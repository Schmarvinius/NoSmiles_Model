{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Idea\n",
    "This notebook tries a approach with existing models to classify laughter in videos.\n",
    "\n",
    "#### Process\n",
    "1. Fetch Data Set (FER2013, RAVDESS, AffectNet)\n",
    "2. Use OpenCV to cut down the images\n",
    "3. Use a CNN to extract features\n",
    "4. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/I569613/Developer/git/university/0_studienarbeit/local/.venv/lib/python3.13/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "Repo card metadata block was not found. Setting CardData to empty.\n",
      "Generating train split: 100%|██████████| 13233/13233 [00:02<00:00, 6518.52 examples/s] \n"
     ]
    }
   ],
   "source": [
    "# # get dataset\n",
    "\n",
    "# from datasets import load_dataset\n",
    "\n",
    "# ds = load_dataset(\"zrthxn/SmilingOrNot\") # this dataset uses binary labels (12k images with 1200 labeled 50/50, 64px)\n",
    "# # ds = load_dataset(\"akomand/celeba-smile\") # this dataset uses text lables (200k images, 178px)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "from torchvision import transforms\n",
    "from torch.utils.data import DataLoader\n",
    "import torch\n",
    "\n",
    "# Laden des SmilingOrNot-Datensatzes (dein ursprünglicher Datensatz)\n",
    "ds_smile = load_dataset(\"zrthxn/SmilingOrNot\")\n",
    "\n",
    "# Laden des AffectNet-Datensatzes\n",
    "ds_affectnet = load_dataset(\"affectnet\", name=\"expression\")\n",
    "\n",
    "# Filtern des AffectNet-Datensatzes nach \"Happiness\"\n",
    "ds_affectnet_happy = ds_affectnet.filter(lambda example: example[\"expression\"] == 3) # 3 represents happiness in affectnet\n",
    "\n",
    "# Transformieren der Bilder\n",
    "transform = transforms.Compose([\n",
    "    transforms.Resize((64, 64)),  # Anpassen an die Größe des SmilingOrNot-Datensatzes\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]) # Normalisierung für vortrainierte CNNs\n",
    "])\n",
    "\n",
    "def transform_images(examples):\n",
    "    images = [transform(image.convert(\"RGB\")) for image in examples[\"image\"]]\n",
    "    return {\"pixel_values\": images}\n",
    "\n",
    "ds_smile = ds_smile.with_transform(transform_images)\n",
    "ds_affectnet_happy = ds_affectnet_happy.with_transform(transform_images)\n",
    "\n",
    "# Erstellen von DataLoaders\n",
    "batch_size = 32\n",
    "train_dataloader_smile = DataLoader(ds_smile[\"train\"], batch_size=batch_size, shuffle=True)\n",
    "test_dataloader_smile = DataLoader(ds_smile[\"test\"], batch_size=batch_size)\n",
    "\n",
    "train_dataloader_affectnet = DataLoader(ds_affectnet_happy[\"train\"], batch_size=batch_size, shuffle=True)\n",
    "val_dataloader_affectnet = DataLoader(ds_affectnet_happy[\"validation\"], batch_size=batch_size)\n",
    "\n",
    "# Erstellen von Labels für AffectNet\n",
    "def create_affectnet_labels(examples):\n",
    "    labels = [1] * len(examples[\"expression\"]) # 1 represents smiling/laughing\n",
    "    return {\"labels\": labels}\n",
    "\n",
    "ds_affectnet_happy = ds_affectnet_happy.map(create_affectnet_labels, batched=True)\n",
    "\n",
    "# Erstellen von Labels für SmilingOrNot\n",
    "def create_smile_labels(examples):\n",
    "    return {\"labels\": examples[\"label\"]}\n",
    "\n",
    "ds_smile = ds_smile.map(create_smile_labels, batched=True)\n",
    "\n",
    "# Erstellen von DataLoaders mit Labels\n",
    "train_dataloader_smile = DataLoader(ds_smile[\"train\"], batch_size=batch_size, shuffle=True)\n",
    "test_dataloader_smile = DataLoader(ds_smile[\"test\"], batch_size=batch_size)\n",
    "\n",
    "train_dataloader_affectnet = DataLoader(ds_affectnet_happy[\"train\"], batch_size=batch_size, shuffle=True)\n",
    "val_dataloader_affectnet = DataLoader(ds_affectnet_happy[\"validation\"], batch_size=batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "import torchvision.models as models\n",
    "\n",
    "model = models.resnet18(pretrained=True)  # Beispiel: ResNet18\n",
    "num_ftrs = model.fc.in_features\n",
    "model.fc = nn.Linear(num_ftrs, 2)  # 2 Klassen: Lachen/Schmunzeln, neutral"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.optim as optim\n",
    "\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
    "\n",
    "num_epochs = 10\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model.to(device)\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    model.train()\n",
    "    for batch in train_dataloader:\n",
    "        inputs = batch[\"pixel_values\"].to(device)\n",
    "        labels = batch[\"labels\"].to(device)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(inputs)\n",
    "        loss = criterion(outputs, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "    # Validierung\n",
    "    model.eval()\n",
    "    val_loss = 0.0\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    with torch.no_grad():\n",
    "        for batch in val_dataloader:\n",
    "            inputs = batch[\"pixel_values\"].to(device)\n",
    "            labels = batch[\"labels\"].to(device)\n",
    "\n",
    "            outputs = model(inputs)\n",
    "            loss = criterion(outputs, labels)\n",
    "            val_loss += loss.item()\n",
    "\n",
    "            _, predicted = outputs.max(1)\n",
    "            total += labels.size(0)\n",
    "            correct += predicted.eq(labels).sum().item()\n",
    "\n",
    "    print(f\"Epoch {epoch+1}/{num_epochs}, Val Loss: {val_loss/len(val_dataloader)}, Val Accuracy: {100*correct/total}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Testen\n",
    "model.eval()\n",
    "test_loss = 0.0\n",
    "correct = 0\n",
    "total = 0\n",
    "with torch.no_grad():\n",
    "    for batch in test_dataloader:\n",
    "        inputs = batch[\"pixel_values\"].to(device)\n",
    "        labels = batch[\"labels\"].to(device)\n",
    "\n",
    "        outputs = model(inputs)\n",
    "        loss = criterion(outputs, labels)\n",
    "        test_loss += loss.item()\n",
    "\n",
    "        _, predicted = outputs.max(1)\n",
    "        total += labels.size(0)\n",
    "        correct += predicted.eq(labels).sum().item()\n",
    "\n",
    "print(f\"Test Loss: {test_loss/len(test_dataloader)}, Test Accuracy: {100*correct/total}%\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
