{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch import nn, optim\n",
    "from torch.utils.data import DataLoader\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "from torchvision import datasets, transforms\n",
    "import os\n",
    "from torch.utils.data import Dataset, ConcatDataset\n",
    "import torch\n",
    "\n",
    "use_gpu = True if torch.cuda.is_available() else False\n",
    "print(use_gpu)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Custom Dataset Classes\n",
    "dataset_dir = os.path.join(os.getcwd(), \"datasets\")\n",
    "\n",
    "transform = transforms.Compose([\n",
    "    transforms.Resize((48, 48)),  # Resize images to 224x224\n",
    "    transforms.ToTensor(),         # Convert images to PyTorch tensors\n",
    "    transforms.Normalize(mean=[0.5, 0.5, 0.5], std=[0.5, 0.5, 0.5])  # Normalize\n",
    "])\n",
    "\n",
    "category_mapping = {\n",
    "    \"not_happy\": 0,\n",
    "    \"happy\": 1\n",
    "}\n",
    "\n",
    "class RecategorizedDataset(Dataset):\n",
    "    def __init__(self, original_dataset, target_class):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            original_dataset: The dataset to wrap (e.g., ImageFolder).\n",
    "            target_class: The class index to map to 1. All other classes will be mapped to 0.\n",
    "        \"\"\"\n",
    "        self.original_dataset = original_dataset\n",
    "        self.target_class = target_class\n",
    "\n",
    "    def __len__(self):\n",
    "        # Return the length of the original dataset\n",
    "        return len(self.original_dataset)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            idx: Index of the sample to retrieve.\n",
    "        Returns:\n",
    "            A tuple (image, new_label), where new_label is 1 if the original label matches\n",
    "            the target_class, otherwise 0.\n",
    "        \"\"\"\n",
    "        # Get the original image and label\n",
    "        image, label = self.original_dataset[idx]\n",
    "        \n",
    "        # Map the label: 1 if it matches the target class, otherwise 0\n",
    "        new_label = 1 if label == self.target_class else 0\n",
    "        \n",
    "        return image, new_label\n",
    "    \n",
    "import random\n",
    "\n",
    "class BalancedDataset(Dataset):\n",
    "    def __init__(self, original_dataset):\n",
    "        self.original_dataset = original_dataset\n",
    "        self.filtered_indices = self._balance_dataset()\n",
    "\n",
    "    def _balance_dataset(self):\n",
    "        # Separate indices for each class\n",
    "        class_indices = {}\n",
    "        for idx, (_, label) in enumerate(self.original_dataset):\n",
    "            if label not in class_indices:\n",
    "                class_indices[label] = []\n",
    "            class_indices[label].append(idx)\n",
    "\n",
    "        # Find the minimum class size\n",
    "        min_class_size = min(len(indices) for indices in class_indices.values())\n",
    "\n",
    "        # Sample indices to balance the dataset\n",
    "        balanced_indices = []\n",
    "        for indices in class_indices.values():\n",
    "            balanced_indices.extend(random.sample(indices, min_class_size))\n",
    "\n",
    "        random.shuffle(balanced_indices)  # Shuffle the balanced dataset\n",
    "        return balanced_indices\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.filtered_indices)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        actual_idx = self.filtered_indices[idx]\n",
    "        return self.original_dataset[actual_idx]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load Datasets\n",
    "# Load AffectNet dataset / \n",
    "# Recategorize AffectNet datasets\n",
    "affectnet_dataset_train = RecategorizedDataset(\n",
    "    datasets.ImageFolder(os.path.join(dataset_dir, \"AffectNet/train\"), transform=transform),\n",
    "    target_class=3\n",
    ")\n",
    "affectnet_dataset_test = RecategorizedDataset(\n",
    "    datasets.ImageFolder(os.path.join(dataset_dir, \"AffectNet/test\"), transform=transform),\n",
    "    target_class=3\n",
    ")\n",
    "\n",
    "affectnet_dataset_val = RecategorizedDataset(\n",
    "    datasets.ImageFolder(os.path.join(dataset_dir, \"AffectNet/val\"), transform=transform),\n",
    "    target_class=3\n",
    ")\n",
    "\n",
    "# Recategorize FER2013 datasets\n",
    "fer2013_dataset_train = RecategorizedDataset(\n",
    "    datasets.ImageFolder(os.path.join(dataset_dir, \"fer2013/train\"), transform=transform),\n",
    "    target_class=3\n",
    ")\n",
    "fer2013_dataset_test = RecategorizedDataset(\n",
    "    datasets.ImageFolder(os.path.join(dataset_dir, \"fer2013/test\"), transform=transform),\n",
    "    target_class=3\n",
    ")\n",
    "\n",
    "# Recategorize RAF-DB datasets\n",
    "raf_db_dataset_train = RecategorizedDataset(\n",
    "    datasets.ImageFolder(os.path.join(dataset_dir, \"raf-db-dataset/train\"), transform=transform),\n",
    "    target_class=3\n",
    ")\n",
    "raf_db_dataset_test = RecategorizedDataset(\n",
    "    datasets.ImageFolder(os.path.join(dataset_dir, \"raf-db-dataset/test\"), transform=transform),\n",
    "    target_class=3\n",
    ")\n",
    "\n",
    "# Merge Datasets\n",
    "# Merge training datasets\n",
    "merged_dataset_train = BalancedDataset(ConcatDataset([affectnet_dataset_train, fer2013_dataset_train, raf_db_dataset_train]))\n",
    "\n",
    "# Merge test datasets\n",
    "merged_dataset_test = BalancedDataset(ConcatDataset([affectnet_dataset_test, fer2013_dataset_test, raf_db_dataset_test]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 1: Load the Pre-Trained EfficientNet Model\n",
    "efficientnet = torch.hub.load('NVIDIA/DeepLearningExamples:torchhub', 'nvidia_efficientnet_b0', pretrained=True)\n",
    "\n",
    "# Step 2: Modify the Model\n",
    "# Replace the final classification layer to match the number of classes in your dataset\n",
    "num_classes = 2  # Example: Binary classification\n",
    "efficientnet.classifier.fc = nn.Linear(efficientnet.classifier.fc.in_features, num_classes)\n",
    "\n",
    "# Move the model to GPU if available\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "efficientnet = efficientnet.to(device)\n",
    "\n",
    "# Step 3: Prepare the Dataset and DataLoader\n",
    "batch_size = 32\n",
    "train_loader = DataLoader(merged_dataset_train, batch_size=batch_size, shuffle=True, num_workers=4, pin_memory=True)\n",
    "test_loader = DataLoader(merged_dataset_test, batch_size=batch_size, shuffle=False, num_workers=4, pin_memory=True)\n",
    "\n",
    "# Step 4: Define the Optimizer, Loss Function, and Scheduler\n",
    "lr = 0.001\n",
    "optimizer = optim.Adam(efficientnet.parameters(), lr=lr)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "scheduler = optim.lr_scheduler.StepLR(optimizer, step_size=5, gamma=0.1)  # Reduce LR every 5 epochs\n",
    "\n",
    "# Step 5: Initialize TensorBoard for Logging\n",
    "writer = SummaryWriter(\"runs/efficientnet_finetune\")\n",
    "\n",
    "# Step 6: Define Validation Function\n",
    "def validate(model, test_loader, criterion):\n",
    "    model.eval()\n",
    "    test_loss = 0.0\n",
    "    correct = 0\n",
    "    total = 0\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for images, labels in test_loader:\n",
    "            images, labels = images.to(device), labels.to(device)\n",
    "            outputs = model(images)\n",
    "            loss = criterion(outputs, labels)\n",
    "            test_loss += loss.item()\n",
    "            _, predicted = torch.max(outputs, 1)\n",
    "            total += labels.size(0)\n",
    "            correct += (predicted == labels).sum().item()\n",
    "\n",
    "    average_loss = test_loss / len(test_loader)\n",
    "    accuracy = 100 * correct / total\n",
    "    print(f\"Validation Loss: {average_loss:.4f}, Accuracy: {accuracy:.2f}%\")\n",
    "    return average_loss, accuracy\n",
    "\n",
    "# Step 7: Fine-Tune the Model\n",
    "num_epochs = 10\n",
    "scaler = torch.cuda.amp.GradScaler(\"cuda\")  # For mixed precision training\n",
    "best_accuracy = 0.0\n",
    "early_stopping_patience = 3\n",
    "no_improvement_epochs = 0\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    efficientnet.train()\n",
    "    running_loss = 0.0\n",
    "    correct = 0\n",
    "    total = 0\n",
    "\n",
    "    for i, (images, labels) in enumerate(train_loader):\n",
    "        images, labels = images.to(device), labels.to(device)\n",
    "\n",
    "        # Forward pass with mixed precision\n",
    "        with torch.cuda.amp.autocast(\"cuda\"):\n",
    "            outputs = efficientnet(images)\n",
    "            loss = criterion(outputs, labels)\n",
    "\n",
    "        # Backward pass and optimization\n",
    "        optimizer.zero_grad()\n",
    "        scaler.scale(loss).backward()\n",
    "        scaler.step(optimizer)\n",
    "        scaler.update()\n",
    "\n",
    "        # Track loss and accuracy\n",
    "        running_loss += loss.item()\n",
    "        _, predicted = torch.max(outputs, 1)\n",
    "        total += labels.size(0)\n",
    "        correct += (predicted == labels).sum().item()\n",
    "\n",
    "        # Print training progress\n",
    "        if (i + 1) % 10 == 0:\n",
    "            print(f\"Epoch [{epoch+1}/{num_epochs}], Step [{i+1}/{len(train_loader)}], \"\n",
    "                  f\"Loss: {loss.item():.4f}, Accuracy: {100 * correct / total:.2f}%\")\n",
    "\n",
    "    # Log training metrics\n",
    "    train_loss = running_loss / len(train_loader)\n",
    "    train_accuracy = 100 * correct / total\n",
    "    print(f\"Epoch [{epoch+1}/{num_epochs}], Loss: {train_loss:.4f}, Accuracy: {train_accuracy:.2f}%\")\n",
    "    writer.add_scalar(\"Loss/Train\", train_loss, epoch)\n",
    "    writer.add_scalar(\"Accuracy/Train\", train_accuracy, epoch)\n",
    "\n",
    "    # Validation\n",
    "    val_loss, val_accuracy = validate(efficientnet, test_loader, criterion)\n",
    "    writer.add_scalar(\"Loss/Validation\", val_loss, epoch)\n",
    "    writer.add_scalar(\"Accuracy/Validation\", val_accuracy, epoch)\n",
    "\n",
    "    # Save the best model\n",
    "    if val_accuracy > best_accuracy:\n",
    "        best_accuracy = val_accuracy\n",
    "        no_improvement_epochs = 0\n",
    "        os.makedirs(\"models\", exist_ok=True)\n",
    "        torch.save(efficientnet.state_dict(), \"models/best_fine_tuned_efficientnet_b0.pth\")\n",
    "        print(f\"Saved Best Model with Accuracy: {best_accuracy:.2f}%\")\n",
    "    else:\n",
    "        no_improvement_epochs += 1\n",
    "\n",
    "    # Early stopping\n",
    "    if no_improvement_epochs >= early_stopping_patience:\n",
    "        print(\"Early stopping triggered.\")\n",
    "        break\n",
    "\n",
    "    # Step the scheduler\n",
    "    scheduler.step()\n",
    "\n",
    "# Close TensorBoard writer\n",
    "writer.close()\n",
    "\n",
    "# Final message\n",
    "print(\"Training complete.\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
